{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sepal length (cm)',\n",
       " 'sepal width (cm)',\n",
       " 'petal length (cm)',\n",
       " 'petal width (cm)']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "iris.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>setosa</th>\n",
       "      <td>0.414988</td>\n",
       "      <td>1.461297</td>\n",
       "      <td>-2.262141</td>\n",
       "      <td>-1.029095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>versicolor</th>\n",
       "      <td>0.416640</td>\n",
       "      <td>-1.600833</td>\n",
       "      <td>0.577658</td>\n",
       "      <td>-1.385538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>virginica</th>\n",
       "      <td>-1.707525</td>\n",
       "      <td>-1.534268</td>\n",
       "      <td>2.470972</td>\n",
       "      <td>2.555382</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            sepal length (cm)  sepal width (cm)  petal length (cm)  \\\n",
       "setosa               0.414988          1.461297          -2.262141   \n",
       "versicolor           0.416640         -1.600833           0.577658   \n",
       "virginica           -1.707525         -1.534268           2.470972   \n",
       "\n",
       "            petal width (cm)  \n",
       "setosa             -1.029095  \n",
       "versicolor         -1.385538  \n",
       "virginica           2.555382  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coeffs = pd.DataFrame(model.coef_, columns=iris.feature_names, index=iris.target_names)\n",
    "coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_norm =  StandardScaler().fit_transform(X)\n",
    "model.fit(X_norm, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>setosa</th>\n",
       "      <td>-0.810166</td>\n",
       "      <td>1.393699</td>\n",
       "      <td>-1.687386</td>\n",
       "      <td>-1.518991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>versicolor</th>\n",
       "      <td>0.130380</td>\n",
       "      <td>-1.246338</td>\n",
       "      <td>0.789195</td>\n",
       "      <td>-0.889440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>virginica</th>\n",
       "      <td>0.012990</td>\n",
       "      <td>-0.144535</td>\n",
       "      <td>1.863173</td>\n",
       "      <td>2.698873</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            sepal length (cm)  sepal width (cm)  petal length (cm)  \\\n",
       "setosa              -0.810166          1.393699          -1.687386   \n",
       "versicolor           0.130380         -1.246338           0.789195   \n",
       "virginica            0.012990         -0.144535           1.863173   \n",
       "\n",
       "            petal width (cm)  \n",
       "setosa             -1.518991  \n",
       "versicolor         -0.889440  \n",
       "virginica           2.698873  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coeffs = pd.DataFrame(model.coef_, columns = iris.feature_names, index =iris.target_names)\n",
    "coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAb8AAAFdCAYAAACTn2P7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0e88c28fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# first import matplotlib\n",
    "% matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# then create a figure and a plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# display the matrix\n",
    "cax = ax.matshow(coeffs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
